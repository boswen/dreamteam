Hello again! It's been about a week since we worked on Alex. I'd definitely like to work on revising Alex's system prompt, as well as the system prompt generator that helped make it -- but not just yet.

Right now I think we need to finish setting up the way for our LLM agents to be able to directly and autonomously interact with other technologies -- such as how Alex needs to be able to use a tool (such as Stable Diffusion [SD]) to create images, or how Darinor needs to be able to be able to design game mechanics and storyboards -- maybe using some of the same tools? I wasn't able to use the tadasant's mcp-server-stability-ai from GitHub, since it was designed to use a web-hosted version of SD and would cost credits each time. However, I found this GitHub project: https://github.com/AUTOMATIC1111/stable-diffusion-webui and this MCP server for it: https://www.magicslides.app/mcps/ichigo3766-stable-diffusion . I think once I get those two options in place, I can  use the ollama mcp bridge to wire up Alex to the local SD Web UI tool.

---
Also, I think I found a project that is very similar to Dream Team called Crew AI (https://github.com/vladimirovpv/crewAI). Per their Github readme, Crew AI is "designed to enable AI agents to assume roles, share goals, and operate in a cohesive unit" and "provides the backbone for sophisticated multi-agent interactions." It looks like their focus might be on business application development, while ours is on video game development. But still, it's worth taking a look! Can you take a look thru their GitHub repo, compare what they're doing to what we *want* to do with Dream Team, and provide me with a short summary (just a few paragraphs) that explains the differences and/or similarities between our two projects?

-----
Next, please take a note for two things to revisit after Alex can make images on "his own" -- i.e., with just our prompting, he can connect to the our local SD, make it generate and image, and then tell me it's done:

1. We need to work on defining his "style" within Stable Diffusion -- there are TONS of settings such as selecting a "Lora" (what even are those?), defining the 'sampling steps', 'sampling method', and so much more. We need to fine-tune this and run some prompts manually within SD to pre-define the settings we want Alex to use beyond just our prompt.

2. Resource limits/management -- We should revisit our conversation in the "Virtual Infrastructure for AI" topic within this "Dream Team" project folder. Once we know Alex *can* make images we may need to *slow down* how fast he makes them to hours instead of minutes so that multiple LLMs can run simultaneously and each can work autonomously on their assigned tasks -- if this is even still the right way to do it. A thought came to mind that perhaps running the models asynchronously in serial (one after the other) instead of in parallel might be a solution - specifically if we can't figure out a way to run multiple models and allow them to sort of organically ebb and flow their usage of RAM and GPU VRAM without using too much and causing an "out of memory" error. Maybe we need to setup a queue so that when a model needs a certain amount of memory, it sends a task to a queue and that queue (maybe it's own mini MCP server/client?) runs the more memory intensive things like image generation one at a time? 


